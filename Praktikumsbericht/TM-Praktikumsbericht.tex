\documentclass[a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage[autostyle=true,german=quotes]{csquotes}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{rotating}
\usepackage{tabularx}
\usepackage[colorlinks, linkcolor = black, citecolor = black, filecolor = black, urlcolor = blue]{hyperref} 
\author{Sebastian Gottwald, Simon Bordewisch}
\title{Text-Mining Praktikumsbericht}
\date{4. Februar 2016}
\begin{document}
\begin{titlepage}
\maketitle
\end{titlepage}

\section{Einleitung}
	Im Rahmen des Moduls Text-Mining hatten wir die Aufgabe einen Klassifikator für den Stanford Named Entity Recognizer (Stanford NER) zu erstellen. 
	Als Datengrundlage stand uns ein, von Studenten erstelltes, Programm zur Verfügung, welches eine kategorisierte Liste von Titel der deutschen Wikipedia erzeugt.
	Aufgabe war es, diese Daten aufzubereiten und anhand von Wikipediaartikeln aus einem XML-Dump Trainingsdaten für den Stanford NER Klassifikator zu erstellen, welcher Personen-, Organisations- und Ortsnamen erkennt.
	\\\\
	Der Stanford NER ist eine Java-Implementation eines Named Entity Recognizers (NER). Ein NER markiert Wort-Sequenzenn in einem Text welche bestimmte Kategorien repräsentieren (z.B. Personen, Orte, Organisationen oder auch Gene und Proteine).
\section{Methodik und Vorgehen}
	\subsection{Auswahl der Vergleichsdaten}
		Insgesamt standen fünf verschiedene Programme des Vorjahrespraktika zur Verfügung, welche wir als Ausgangsdaten zur Erstellung der Klassifikators nutzen sollten.
		Die Entscheidung fiel auf das Programm, welches auf den Titeln der Wikipediaartikel arbeitet, da dieses eine umfangreiche und nahezu fehlerfreie Liste von Organisationen, sowie Personen- und Ortsnamen liefert.  
		Die anderen Programme waren schlecht oder gar nicht dokumentiert beziehungsweise ohne weiteres Zutun nicht lauffähig. 
		Das Programm ''Wiki\_ORG'' arbeitet zum Beispiel auf einer Datenbank welche nicht mehr vorhanden ist und im Programm ''WIKI\_ORT'' haben die zusätzlichen Abhängigketien gefehlt
		
	\subsection{Extraktion der Wikipedia Artikel}
		Als Datengrundlage zur Erstellung der Trainingsdaten des Klassifikators, diente uns der aktuelle Wikipedia Dump ''dewiki-latest-pages-articles.xml.bz2''. 
		Zum extrahieren der Daten wurde die StAX-API verwendet, da diese sich gut für große XML-Datenmengen eignet. 
		Beim parsen des Wikipediadumps wurden die Normdaten der Artikel überprüft, da Personenartikel die Bezeichnung ''Typ=p'', Ortsartikel die Bezeichung ''Typ=g'' und Körperschaftsartikel die Bezeichnung ''Typ=k'' in ihren Normdaten besitzen und somit die Artikel extrahieren werden können, welche wahrscheinlich beste Ergebnisse beim finden von Wörtern liefert. 
		Zur Bereinigung der extrahierten Artikel wurde das Python-Script ''WikiExtraktor.py''\footnote{Quelle: \url{https://github.com/attardi/wikiextractor}} verwendet, welches den Klartext der Wikipediaartikel im Ordner ''Ergebnisse/AA/wiki\_00'' abspeichert.
	\subsection{Suche der Entities anhand von Vergleichsdaten}
	
	\subsection{Erstellung der Traningsdaten}
		Zur Erstellung der Trainingsdaten, teilten wir zunächst unseren markierten Text in einzelne Wörter auf. 
		Dafür benutzten wir den von der Stanford Core NLP mitgelieferten Tokenizer. 
		Dieser Transformiert den Text dahingehend, dass er in eine Zeile ein Wort schreibt. 
		Wir fügten danach an jedes Wort einen Tabulator und den Buchstaben 'O' welcher als Default Buchstabe des Stanford NER genutzt wird.
		Im nächsten Schritt parsten wir den entstandenen tokenisierten Text und ersetzten den Buchstaben 'O' der Wörter zwischen den Tags, durch die jeweilige Kategorie. 
		In diesem Schritt löschten wir die tags aus dem Text und schrieben das Ergebnis, also die richtig formatierten Trainingsdaten, in die Datei ''TrainingsData.out''.
	\subsection{Erstellung des Klassifikators}
		In diesem Schritt trainierten wir den Stanford NER anhand unserer maschinell erstellten Trainingsdaten. 
		Dafür rufen wir die Main Methode des Stanford NER Klassifikators auf und übergeben ihm eine Propertydatei (''Ressourcen/default.prop''). 
		Diese gibt Eigenschaften des Trainings und des zu erstellenden Klassifikators an.
\section{Probleme bei der Lösung der Aufgabe}
	Beim extrahieren des Klartextes aus den gefilterten Artikeln mit dem Programm ''WikiExtraktor.py'' werden zum bei manchen Artikeln Teile des Textes verworfen. 
	Wir konnten leider die Ursache von diesem Fehler nicht finden. 
	Da wir jedoch auf einer sehr großen Menge an Daten arbeiten, entschieden wir uns diesen Bug zu ignorieren, da das Training auf einem Teilartikel wahrscheinlich keine Auswirkung auf unser Ergebnis hat.
	
	Zum Auffinden der zu markierenden Wörter in unserem Text benutzten wir zunächt ''exact-matches''. Vornamen und Nachnamen die einzeln im Text standen und die Beugung von Wörtern wurden dadurch leider nicht erkannt. 
	Um dies zu verbessern, spalteten wir die Liste der Personennamen in Vornamen und Nachnamen auf und benutzten die Levenstein Distanz um Beugungen zu erkennen. 
	Hierbei ergab sich ein erneutes Problem die auf die Mehrdeutigkeit von Wörtern, speziell Nachnamen, zurückzuführen ist, denn nun wurden Wörter markiert welche eine andere Beudeutung haben. 
	Um dies zu verhindern wurde eine Art Bestrafung bei der Erkennung von Nachnamen eingeführt, wenn kein Vornamen vorhanden ist. 
	Generell ist die Mehrdeutigkeit von Wörtern ein großes Problem beim markieren und wir erlangen somit viele falsch positive Ergbnisse. 
	Desweiteren werden durch die Levenstein Distanz ebenfalls viele Wörter auch falsch markiert. 
	Als Beispiel dient das Wort ''Triumph'' welches sowohl eine Motorradmarke also Organisation, als auch Triumph im Sinne von Siegen ist. Das Wort ''Stadt'' wird als Ort markiert wird, da es einen Ort Namens ''Stade'' gibt und die Leventstein Distanz lediglich 1 beträgt.
	
	Ein weiteres Problem stellt die Laufzeit unseres Programmes dar. 
	Die zwei am längsten dauernden Funtionen sind das Extrahieren der Wikipediaartikel und das erstellen des Klassifikators mit der Stanford NER Bibliothek.
\section{Ergebnisse}
	Wir haben all unsere Klassifizierer auf einem per Hand annotierten Goldstandart verglichen. 
	Dieser Goldstandart bestand aus 100 Beispielsätzen (1530 Wörter und Satzzeichen) aus Nachrichtenartikeln oder Beispielsätzen des Projektes ''Wortschatz'' der Universität Leipzig. Desweiteren haben wir unsere Klassifikator gegen den auf der Stanford NER Webseite erhältlichen deutschen Klassifikator  verglichen.

\newpage

\begin{sidewaystable}
\label{Tabelle Ergebnisse}
\caption{Ergebnistabelle}
\begin{longtable}{|c|c|c|c|c|c|c|c|c|}
\hline 
Anzahl Artikel & Variante & Property Datei & Precision & Recall & F1 & TP & FP & FN \\ 
\hline 
150 & Personen Vor- Nachname getrennt & german.dewac\_175m\_600 & 40,98\% & 47,19\% & 43,86\% & 84 & 121 & 94 \\ 
\hline 
150 & Personen Vollnamen & german.dewac\_175m\_600 & 50,94\% & 30,34\% & 38,03\% & 54 & 52 & 124 \\ 
\hline 
150 & Personen Vor- Nachname getrennt & default & 39,11\% & 44,38\% & 41,58\% &  79 & 123 & 99 \\ 
\hline 
150 & Personen Vollname & default & 49,53\% & 29,78\% & 37,19\% & 51 & 54 & 125 \\ 
\hline 
  &  & German-Classifier-DEWAC & 65,81\% & 57,30\% & 61,26\% & 102 & 53 & 76 \\ 
\hline 
  &  & German-Classifier-HGC & 61,22\% & 50,56\% & 55,38\% & 90 & 57 & 88 \\ 
\hline
\end{longtable} 
\end{sidewaystable}

\end{document}
