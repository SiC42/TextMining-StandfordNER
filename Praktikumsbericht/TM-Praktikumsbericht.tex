\documentclass[a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage[autostyle=true,german=quotes]{csquotes}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[colorlinks, linkcolor = black, citecolor = black, filecolor = black, urlcolor = blue]{hyperref} 
\author{Sebastian Gottwald, Simon Bordewisch}
\title{Text-Mining Praktikumsbericht}
\date{4. Februar 2016}
\begin{document}
\maketitle
\newpage
\section{Einleitung}
Im Rahmen des Moduls Text-Mining hatten wir die Aufgabe einen Klassifikator für den Stanford Named Entity Recognizer (Stanford NER) zu erstellen. Als Datengrundlage standen uns ein, von Studenten erstelltes, Programm zur Verfügung, welches eine kategorisierte Liste von Titel der deutschen Wikipedia erzeugt. Unsere Aufgabe war es, diese Daten aufzubereiten und anhand von Wikipediaartikeln aus einem XML-Dump, Trainingsdaten für den Stanford NER Klassifikator zu erstellen, welcher Personen-, Organisations- und Ortsnamen erkennt
\\\\
Der Stanford NER ist eine Java-Implementation eines Named Entity Recognizers (NER). Ein NER markiert Wort-Sequenzenn in einem Text welche bestimmte Kategorien repräsentieren (z.B. Personen, Orte, Organisationen oder auch Gene und Proteine).
\section{Methodik und Vorgehen}
\subsection{Aufarbeitung der Vergleichsdaten}
Wir hatten insgesamt vier verschiedene Programme des Vorjahrespraktikas zur Verfügung. Wir entschieden uns das Programm zu nutzen, welches auf den Titeln der Wikipediaartikel arbeitet, da dieses am leichtesten zu bedienen war und wir eine umfangreiche und nahezu fehlerfreie Liste von Organisationen sowie Personen- und Ortsnamen erhielten. Wir mussten zunächst diese Liste aufarbeiten, sodass sie ein für uns nutzbares Format besitzt. Wir speicherten die einzelnen Entitäten in einer TreeMap zur weiterverarbeitung. Dadurch erziehlen wir gute Zeiten bei der Suche nach einem Wort.
\subsection{Extraktion der Wikipedia Artikel}
Als Datengrundlage zur erstellung der Trainingsdaten des Klassifikators, diente uns der aktuelle Wikipedia Dump ''dewiki-latest-pages-articles.xml.bz2''. Zum extrahieren der Daten nutzten wir StAX-API da diese sich gut für Große Datenmengen eignet. Beim parsen de Wikipediadumps haben wir die Normdaten der Artikel überprüft, da Personenartikel die Bezeichnung ''Typ=p'', Ortsartikel die Bezeichung ''Typ=g'' und Körperschaftsartikel die Bezeichnung ''Typ=k'' in ihren Normdaten besitzen und wir somit die Artikel extrahieren können, welche uns wahrscheinlich beste Ergebnisse beim finden von Wörtern liefern. Zur Bereinigung der extrahierten Artikel verwendeten wir das Python-Script ''WikiExtraktor.py'' (Quelle: \url{https://github.com/attardi/wikiextractor}), welches den Klartext der Wikipediaartikel im Ordner ''Ergebnisse/AA/wiki\_00'' abspeichert.

\subsection{Suche der Entities anhand von Vergleichsdaten}
\subsection{Erstellung der Traningsdaten}
Zur Erstellung der Trainingsdaten, teilten wir zunächst unseren markierten Text in einzelne Wörter auf. Dafür benutzten wir den von der Stanford Core NLP mitgelieferten Tokenizer. Dieser Transformiert den Text dahingehend, dass er in eine Zeile ein Wort schreibt. Wir fügten danach an jedes Wort einen Tabulator und den Buchstaben 'O' welcher als Default Buchstabe des Stanford NER genutzt wird.
Im nächsten Schritt parsten wir den entstandenen tokenisierten Text und ersetzten den Buchstaben 'O' der Wörter zwischen den Tags, durch die jeweilige Kategorie. In diesem Schritt löschten wir die tags aus dem Text und schrieben das Ergebnis, also die richtig formatierten Trainingsdaten, in die Datei ''TrainingsData.out''.

\subsection{Erstellung des Klassifikators}
In diesem Schritt trainierten wir den Stanford NER anhand unserer maschinell erstellten Trainingsdaten. Dafür rufen wir die Main Methode des Stanford NER Klassifikators auf und übergeben ihm eine Propertydatei (''Ressourcen/default.prop''). Diese gibt Eigenschaften des Trainings und des zu erstellenden Klassifikators an.

\section{Probleme bei der Lösung der Aufgabe}
Zum Beginn unseres Projektes hatten insgesamt 5 verschiedene Projekte zur Verfügung, welche wir als Ausgangsdaten zur erstellung der Klassifikators nutzen sollten. Diese Programme waren leider zu Teil schlecht oder gar nicht dokumentiert, beziehungsweise nicht lauffähig. Zum Beispiel arbeitete das Programm ''Wiki\_ORG'' auf einer Datenbank welche wir erst hätten aufsetzen müssen, und  das Programm ''WIKI\_ORTE'' ging nicht zu compilieren. Wir entschieden uns deshalb das Programm ''WIKI\_TITLE'' zu nutzen da es die für uns nötigen Daten zur Verfügung stellte und wir es nach ein paar Anpassungen ausführen konnten.

Beim extrahieren des Klartextes aus den gefilterten Artikeln mit dem Programm ''WikiExtraktor.py'' (Quelle: \url{https://github.com/attardi/wikiextractor}) werden zum bei manchen Artikeln Teile des Textes verworfen. Wir konnten leider die Ursache des Bugs nicht finden. Da wir jedoch auf einer sehr großen Menge an Daten arbeiten, entschieden wir uns diesen Bug zu ignorieren, da das Training auf einem Teilartikel wahrscheinlich keine Auswirkung auf unser Ergebnis hat.

Zum Auffinden der zu markierenden Wörter in unserem Text benutzten wir zunächt ''exact-matches''. Vornamen und Nachnamen die einzeln im Text standen und die Beugung von Wörtern wurden dadurch leider nicht erkannt. Um dies zu verbessern, spalteten wir die Liste der Personennamen in Vornamen und Nachnamen auf und benutzten die Levenstein Distanz um Beugungen zu erkennen. Hierbei ergab sich ein erneutes Problem die auf die Mehrdeutigkeit von Wörtern, speziell Nachnamen, zurückzuführen ist, denn nun wurden Wörter markiert welche eine andere Beudeutung haben. Um dies zu verhindern wurde eine Art Bestrafung bei der Erkennung von Nachnamen eingeführt, wenn kein Vornamen vorhanden ist. Generell ist die Mehrdeutigkeit von Wörtern ein großes Problem beim markieren und wir erlangen somit viele falsch positive Ergbnisse. Desweiteren werden durch die Levenstein Distanz ebenfalls viele Wörter auch falsch markiert. Als Beispiel dient das Wort ''Triumph'' welches sowohl eine Motorradmarke also Organisation, als auch Triumph im Sinne von Siegen ist. Das Wort ''Stadt'' wird als Ort markiert wird, da es einen Ort Namens ''Stade'' gibt und die Leventstein Distanz lediglich 1 beträgt.

Ein weiteres Problem stellt die Laufzeit unseres Programmes dar. Die zwei am längsten dauernden Funtionen sind das Extrahieren der Wikipediaartikel und das erstellen des Klassifikators mit der Stanford NER Bibliothek.
\section{Ergebnisse}
\end{document}